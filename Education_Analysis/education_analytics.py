# -*- coding: utf-8 -*-
"""Education Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dIoH-71t2Ld7_bsdtXFby-U2wMBvTYY9

# Solving Edutech Company Problems

## Preparation

### Install the required libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""### Preparing the data"""

url = "https://drive.google.com/uc?id=1hfXQbYuWfJ2E_1TMVu5ebYOnYhXxGR0C"
edu_df = pd.read_csv(url, sep = ";")

edu_df.head(5)

"""# Data Understanding"""

edu_df.info()

"""Tidak terlihat ada anomali seperti ketidaksesuaian tipe data dan lainnya."""

edu_df.isnull().sum()

"""Tidak ada kolom yang mempunyai nilai null, sehingga data sudah bersih dari nilai null."""

edu_df.duplicated().sum()

"""Tidak ada data yang terduplikasi"""

#Numerical Feature Distribution
num_features = edu_df.select_dtypes(include=[np.number])
n_cols = 4
n_rows = (len(num_features.columns) + n_cols - 1) // n_cols

plt.figure(figsize=(16, 12))
for i, column in enumerate(num_features.columns, 1):
    plt.subplot(n_rows, n_cols, i)
    sns.histplot(edu_df[column], bins=30, kde=True, color='blue')
    plt.title(f'Distribution {column}')
plt.tight_layout()
plt.show()

# Categorical Feature Distribution
cat_features = edu_df.select_dtypes(include=[object])
plt.figure(figsize=(14, 8))
for i, column in enumerate(cat_features.columns, 1):
    plt.subplot(2, 4, i)
    sns.countplot(y=edu_df[column], palette='viridis')
    plt.title(f'Distribution {column}')
plt.tight_layout()
plt.show()

"""There is an imbalance between the dropout and enrolled data compared to the graduate data, which can be addressed by applying oversampling methods to balance the dataset."""

edu_df.describe()

"""Nothing looks weird / anomaly

# Data Preparation / Preprocessing
"""

edu_df.columns

"""Average Grade Student"""

edu_df['Curricular_units_2nd_sem_grad'] = edu_df['Curricular_units_2nd_sem_grade'].round(2)

edu_df['Curricular_units_1st_sem_grade'] = edu_df['Curricular_units_1st_sem_grade'].round(2)

edu_df["Average_Grade"] = edu_df["Curricular_units_1st_sem_grade"] + edu_df["Curricular_units_2nd_sem_grade"]
edu_df["Average_Grade"] = edu_df["Average_Grade"] / 2
edu_df["Average_Grade"] = edu_df["Average_Grade"].round(2)

"""Calculating the average grade to determine whether there is a trend or correlation with a student dropping out or not."""

edu_df.to_csv("edu_df.csv", index=False)

"""Convert it into a CSV format for dashboard analysis by stakeholders."""

edu_df.columns

"""# EDA"""

numerical_columns = edu_df.select_dtypes(include=[np.number]).columns

def categorical_plot(features, df, segment_feature=None):
    fig, ax = plt.subplots(len(features), 1,figsize=(10,20))
    for i, feature in enumerate(features):
        if segment_feature:
            sns.countplot(data=df, y=segment_feature, hue=feature, ax=ax[i])
        else:
            sns.countplot(data=df, x=feature, ax=ax[i])
    plt.tight_layout()
    plt.show()

categorical_plot(
    features=[
        'Gender',
        'Marital_status',
        'Scholarship_holder',
        'Debtor',
        'Displaced',
        'Course'
    ],
    df=edu_df,
    segment_feature="Status"
)

"""Categorical Variable Analysis

a. Gender

Male students (1) are more dominant in the Graduate category, while female students (0) are more evenly distributed between Dropout and Graduate.

This suggests that the graduation rate appears to be slightly higher for male students in this dataset.

b. Marital Status

The majority of students fall into the single category (1).

Married students (2) and other categories are relatively few; however, married students tend to appear more often in the Graduate category.

This may indicate that married students are more stable in terms of motivation or financial conditions.

c. Scholarship Holder

Students who receive scholarships (1) have a larger proportion in the Graduate category compared to Dropout.

This indicates that scholarships are positively correlated with graduation rates — possibly because financial support helps students complete their studies.

d. Debtor

Students without debt (0) dominate both the Graduate and Dropout categories, but students with debt (1) tend to appear more frequently in the Dropout category.

This suggests that financial burden may increase the risk of dropping out.

e. Displaced

Students who live away from home (1) include many who graduate, but also many who drop out.

This pattern suggests that being away from home has two sides — some students succeed due to increased independence, while others struggle due to adaptation challenges.

f. Course

The distribution across courses varies significantly.

Some programs (certain course codes) show a higher number of Graduate students, while other programs tend to have higher Dropout rates.
"""

def average_plot(features, df, segment_feature):
    fig, ax = plt.subplots(len(features), 1, figsize=(10, 20))

    for i, feature in enumerate(features):
        sns.barplot(
            data=df,
            x=segment_feature,
            y=feature,
            estimator=np.mean,
            errorbar=('ci', 95),
            ax=ax[i],
            palette="viridis"
        )
        ax[i].set_title(f'Average {feature} by {segment_feature}')
        ax[i].set_ylabel('Average Value')
        ax[i].set_xlabel(segment_feature)

    plt.tight_layout()
    plt.show()

average_plot(
    features=[
        'Admission_grade',
        'Curricular_units_1st_sem_approved',
        'Curricular_units_1st_sem_grade',
        'Curricular_units_2nd_sem_grade'
    ],
    df=edu_df,
    segment_feature="Status"
)

"""Numerical Variable Analysis (Average Plot)

a. Admission Grade

The average admission grade of Graduate students is slightly higher than that of Dropout students.

This suggests that initial academic quality (admission grade) plays a role in study success, although the difference is not very pronounced.

b. Curricular Units 1st Semester (Approved)

Graduate students have a much higher average number of approved courses in the first semester compared to Dropout students.

Dropout students pass on average only about 2–3 courses, while Graduate students pass more than 6.

This is a strong indicator that early-semester performance is a key determinant of graduation.

c. Curricular Units 1st Semester (Grade)

The average first-semester grades of Graduate students are also significantly higher (~12–13) compared to Dropout students (~7–8).

This pattern reinforces the previous finding: performance in the first semester is an early determinant of academic success.

d. Curricular Units 2nd Semester (Grade)

The pattern is consistent with the first semester: Graduate students have higher average grades (~13) compared to Dropout students (~6).

This indicates that academic consistency across the first two semesters is a major factor in completing the study program.
"""

course_status = edu_df.groupby(['Course', 'Status']).size().reset_index(name='Count')
total_per_course = course_status.groupby('Course')['Count'].sum().reset_index(name='Total')
course_status = pd.merge(course_status, total_per_course, on='Course')
course_status['Percentage'] = ((course_status['Count'] / course_status['Total']) * 100).round(2)
course_status.head(10)

plt.figure(figsize=(12, 6))
sns.barplot(
    data=course_status[course_status['Status'] == 'Dropout'],
    x='Course',
    y='Percentage',
    order=course_status[course_status['Status'] == 'Dropout']
            .sort_values('Percentage', ascending=False)['Course'],
    palette='Reds_r'
)
plt.xticks(rotation=45, ha='right')
plt.title('Persentase Dropout per Program Studi', fontsize=14)
plt.ylabel('Dropout Rate (%)')
plt.xlabel('Program Studi')
plt.tight_layout()
plt.show()

"""It can be observed that study program 33 has the highest dropout rate compared to other study programs, which may be an important contributing factor."""

plt.figure(figsize=(35,35))
sns.heatmap(edu_df[numerical_columns].corr(), annot=True, cmap='jet', linecolor='black', linewidth=1)
plt.show()

"""From the data visualization above, several features show positive and negative correlations with each other, indicating the presence of multicollinearity in the dataset. This issue will be addressed during the feature selection stage using forward selection.

# Modeling

### Split
"""

from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(
    edu_df,
    test_size=0.10,
    random_state=42,
    stratify=edu_df["Status"]
)

X_train = train_df.drop("Status", axis=1)
y_train = train_df["Status"]

X_test = test_df.drop("Status", axis=1)
y_test = test_df["Status"]

"""### Encoding"""

from sklearn.preprocessing import LabelEncoder,OrdinalEncoder, MinMaxScaler

#target
le = LabelEncoder()
y_train_enc = le.fit_transform(y_train)
y_test_enc  = le.transform(y_test)

# fitur kategorikal
cat_cols = X_train.select_dtypes(include="object").columns.tolist()

encoder = OrdinalEncoder(
    handle_unknown="use_encoded_value",
    unknown_value=-1
)

X_train_enc = X_train.copy()
X_test_enc  = X_test.copy()

X_train_enc[cat_cols] = encoder.fit_transform(X_train[cat_cols])
X_test_enc[cat_cols]  = encoder.transform(X_test[cat_cols])

"""### Feature Importances"""

from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.ensemble import RandomForestClassifier

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.metrics import f1_score
from sklearn.preprocessing import MinMaxScaler

feature_counts = [8, 10, 12, 15]
results = []

for n in feature_counts:
    rf_fast = RandomForestClassifier(
        n_estimators=20,
        max_depth=10,
        random_state=42,
        n_jobs=-1
    )

    sfs = SequentialFeatureSelector(
        rf_fast,
        n_features_to_select=n,
        direction="forward",
        scoring="f1_macro",
        cv=3,
        n_jobs=-1
    )

    sfs.fit(X_train_enc, y_train_enc)
    selected_features = X_train_enc.columns[sfs.get_support()].tolist()

    X_train_sel = X_train_enc[selected_features]
    X_test_sel = X_test_enc[selected_features]

    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train_sel)
    X_test_scaled = scaler.transform(X_test_sel)

    model = RandomForestClassifier(
        n_estimators=200,
        random_state=42,
        n_jobs=-1
    )

    model.fit(X_train_scaled, y_train_enc)
    y_pred = model.predict(X_test_scaled)

    f1 = f1_score(y_test_enc, y_pred, average="macro")

    results.append({
        "Fitur": n,
        "F1-Score": round(f1, 4),
        "Features": selected_features
    })

df_comparison = pd.DataFrame(results)
print(df_comparison[["Fitur", "F1-Score"]])

best_idx = df_comparison["F1-Score"].idxmax()
recommended_features = df_comparison.loc[best_idx, "Features"]

print("Selected features:", recommended_features)
print("Jumlah fitur:", len(recommended_features))

"""Forward feature selection reduced the feature space from 36 to 12 variables, retaining the most informative academic, demographic, and financial factors relevant to student dropout and graduation.

###Scaling
"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

X_train_sel = X_train_enc[recommended_features]
X_test_sel  = X_test_enc[recommended_features]

X_train_scaled = scaler.fit_transform(X_train_sel)
X_test_scaled  = scaler.transform(X_test_sel)

"""### Oversampling"""

sns.countplot(x=y_train_enc)
plt.title("Class Distribution Before SMOTE")
plt.show()

from imblearn.over_sampling import SMOTE
from collections import Counter

smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(
    X_train_scaled,
    y_train_enc
)

sns.countplot(x=y_train_res)
plt.title("Class Distribution After SMOTE")
plt.show()

print("Distribution after SMOTE:", Counter(y_train_res))
print("Training data after SMOTE:", X_train_res.shape)

"""Oversampling has been applied, and the training data is now balanced.

The algorithms used in this study are more inclined toward Random Forest, XGBoost, and Gradient Boosting. Therefore, I will experiment with and use three of these algorithms.

##Algorithm

#### 1. Random Forest
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

# Baseline model
rf_model = RandomForestClassifier(
    random_state=123,
    n_jobs=-1
)

# Grid parameter
param_grid_fast = {
    'n_estimators': [200, 300],
    'max_depth': [6, 8, None],
    'max_features': ['sqrt'],
    'criterion': ['gini']
}

# GridSearchCV
CV_rdf = GridSearchCV(
    estimator=rf_model,
    param_grid=param_grid_fast,
    cv=3,
    n_jobs=-1,
    scoring='f1_macro',
    verbose=1
)

CV_rdf.fit(X_train_res, y_train_res)

# Best result
print("Best Parameters:", CV_rdf.best_params_)
print("Best F1 Macro Score:", CV_rdf.best_score_)

# Best model
best_rf_model = CV_rdf.best_estimator_

y_pred = best_rf_model.predict(X_test_scaled)

print("\nClassification Report:")
print(classification_report(y_test_enc, y_pred))

y_pred_rf = best_rf_model.predict(X_test_scaled)

print("Classification Report (Random Forest - Tuned):")
print(classification_report(y_test_enc, y_pred_rf))

"""#### 2. XGBoost"""

from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import classification_report
from scipy.stats import randint, uniform

# XGBoost model
xgb_model = XGBClassifier(
    random_state=123,
    objective='multi:softprob',
    num_class=3,
    eval_metric='mlogloss',
    use_label_encoder=False
)

# Parameter search
param_dist = {
    'n_estimators': randint(100, 300),
    'max_depth': randint(3, 7),
    'learning_rate': uniform(0.05, 0.2),
    'subsample': uniform(0.8, 0.2),
    'colsample_bytree': uniform(0.8, 0.2)
}

# RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_dist,
    n_iter=20,
    scoring='f1_macro',
    cv=3,
    n_jobs=-1,
    random_state=123,
    verbose=1
)

# TRAIN
random_search.fit(X_train_res, y_train_res)

# Best result
print("Best Parameters:", random_search.best_params_)
print("Best F1 Macro Score:", random_search.best_score_)

# Best model
best_xgb_model = random_search.best_estimator_

# TEST
y_pred_xgb = best_xgb_model.predict(X_test_scaled)

print("\nClassification Report (XGBoost - Tuned):")
print(classification_report(y_test_enc, y_pred_xgb))

best_xgb_model = random_search.best_estimator_

# data test prediction (scaled & encoded)
y_pred_xgb = best_xgb_model.predict(X_test_scaled)

print("\nClassification Report (XGBoost - Best Params):")
print(classification_report(y_test_enc, y_pred_xgb))

"""#### 3. Gradient Boosting"""

from sklearn.ensemble import GradientBoostingClassifier

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report

# Gradient Boosting Initiation
gboost_model = GradientBoostingClassifier(random_state=123)

# Grid parameter
param_grid = {
    'n_estimators': [150, 250],
    'learning_rate': [0.05, 0.1],
    'max_features': ['sqrt', None],
    'subsample': [0.8, 1.0]
}

# GridSearchCV
CV_gboost = GridSearchCV(
    estimator=gboost_model,
    param_grid=param_grid,
    cv=3,
    n_jobs=-1,
    scoring='f1_macro',
    verbose=1
)

# TRAIN → data hasil SMOTE
CV_gboost.fit(X_train_res, y_train_res)

# Best Result
print("Best Parameters:", CV_gboost.best_params_)
print("Best F1 Macro Score:", CV_gboost.best_score_)

# Best Model
best_gboost_model = CV_gboost.best_estimator_

# TEST → real data test (scaled)
y_pred_gboost = best_gboost_model.predict(X_test_scaled)

print("\nClassification Report (Gradient Boosting - Tuned):")
print(classification_report(y_test_enc, y_pred_gboost))

best_gboost_model = GradientBoostingClassifier(
    learning_rate=0.05,
    max_features='sqrt',
    n_estimators=250,
    subsample=1.0,
    random_state=123
)
best_gboost_model.fit(X_train_res, y_train_res)

y_pred_gb = best_gboost_model.predict(X_test_scaled)

print("\nClassification Report (Gradient Boosting - Tuned):")
print(classification_report(y_test_enc, y_pred_gb))

"""## Evaluation"""

from sklearn.metrics import confusion_matrix
def evaluating(model_name, y_true, y_pred, labels=None, label_names=None):
    print(f"Model: {model_name}")

    # Default labels
    if labels is None:
        labels = sorted(list(set(y_true)))
    if label_names is None:
        label_names = [str(l) for l in labels]

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=label_names))

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    cm_df = pd.DataFrame(cm, index=label_names, columns=label_names)

    plt.figure(figsize=(5, 4))
    sns.heatmap(cm_df, annot=True, fmt='d', cmap='YlGnBu', cbar=False)
    plt.title(f'Confusion Matrix - {model_name}', fontsize=13)
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

    return cm_df

labels = [0, 1, 2]
label_names = ['Graduate (0)', 'Dropout (1)', 'Enrolled (2)']

cm_rf  = evaluating("Random Forest", y_test_enc, y_pred_rf, labels, label_names)
cm_xgb = evaluating("XGBoost", y_test_enc, y_pred_xgb, labels, label_names)
cm_gb  = evaluating("Gradient Boosting", y_test_enc, y_pred_gb, labels, label_names)

from sklearn.metrics import f1_score, accuracy_score

classes = ['Graduate', 'Dropout', 'Enrolled']

# Gradient Boosting
f1_gb = f1_score(y_test_enc, y_pred_gb, average=None)
f1_gb_avg = f1_score(y_test_enc, y_pred_gb, average='macro')
acc_gb = accuracy_score(y_test_enc, y_pred_gb)

# XGBoost
f1_xgb = f1_score(y_test_enc, y_pred_xgb, average=None)
f1_xgb_avg = f1_score(y_test_enc, y_pred_xgb, average='macro')
acc_xgb = accuracy_score(y_test_enc, y_pred_xgb)

# Random Forest
f1_rf = f1_score(y_test_enc, y_pred_rf, average=None)
f1_rf_avg = f1_score(y_test_enc, y_pred_rf, average='macro')
acc_rf = accuracy_score(y_test_enc, y_pred_rf)

performance_table = pd.DataFrame({
    'Class': classes + ['Average F1', 'Accuracy'],
    'Gradient Boosting': list(f1_gb) + [f1_gb_avg, acc_gb],
    'XGBoost': list(f1_xgb) + [f1_xgb_avg, acc_xgb],
    'Random Forest': list(f1_rf) + [f1_rf_avg, acc_rf]
})

print(performance_table)

"""Based on the evaluation results, XGBoost can be considered the best overall model because:

- It achieves the highest accuracy (≈ 76.7%) among all evaluated models.

- It records the highest average (macro) F1-score (≈ 0.70), indicating the most balanced performance across all three classes.

- It demonstrates superior performance on the minority class (Dropout) compared to both Gradient Boosting and Random Forest, while maintaining strong predictive performance on the Graduate and Enrolled classes.

Although Gradient Boosting and Random Forest show competitive and relatively stable results, their macro F1-scores and accuracy are consistently lower than those of XGBoost. Therefore, XGBoost provides the most reliable and balanced classification performance for this dataset.
"""

import joblib

joblib.dump(best_xgb_model, "xgb_model.pkl")

joblib.dump(scaler, "scaler.pkl")

joblib.dump(recommended_features, "features.pkl")

def plot_feature_importances(feature_importances, feature_names, top_n=15):
    features = pd.DataFrame({
        'Feature': feature_names,
        'Importance': feature_importances
    }).sort_values(by='Importance', ascending=False).head(top_n)

    plt.figure(figsize=(8, 5))
    sns.barplot(
        data=features,
        x='Importance',
        y='Feature'
    )
    plt.title(f'Top {top_n} Feature Importances (XGBoost)')
    plt.xlabel('Importance Score')
    plt.ylabel('Feature')
    plt.tight_layout()
    plt.show()

    return features


# feature importance XGBoost
plot_feature_importances(
    best_xgb_model.feature_importances_,
    recommended_features,
    top_n=10
)